unit NNOptimizers;

interface

{$I NNConfig.inc}

uses
  WinAPI.Windows,
  System.Types, System.Classes, System.SysUtils, System.Math,
  NNCommon, NeuralNetwork;

type
  TNNSGD = class( TNNOptimizer )
  public
    constructor Create; override;
    procedure UpdateParameters(AWeights, ABiases, AWeightGrads, ABiasGrads: PNNFloat;
                              AWeightCount, ABiasCount: int32); override;
  end;

  TNNAdam = class(TNNOptimizer)
  private
    FInited: boolean;
    FWeightsM, FWeightsV: TNNDynArray; // Моменты для весов
    FBiasesM, FBiasesV: TNNDynArray;   // Моменты для смещений
    FWeightCount, FBiasCount: int32; // Размеры массивов
    FT: int32;                      // Счётчик шагов
    FBeta1, FBeta2, FEpsilon: NNFloat; // Гиперпараметры
  public
    constructor Create; override;
    procedure Clear; override;
    procedure UpdateParameters(AWeights, ABiases, AWeightGrads, ABiasGrads: PNNFloat;
                              AWeightCount, ABiasCount: int32); override;
  end;

  TNNRMSProp = class(TNNOptimizer)
  private
    FInited: boolean;
    FWeightsV: TNNDynArray; // Среднее квадратов градиентов для весов
    FBiasesV: TNNDynArray;  // Среднее квадратов градиентов для смещений
    FWeightCount, FBiasCount: int32; // Размеры массивов
    FRho, FEpsilon: NNFloat; // Гиперпараметры
  public
    constructor Create; override;
    procedure Clear; override;
    procedure UpdateParameters(AWeights, ABiases, AWeightGrads, ABiasGrads: PNNFloat;
                              AWeightCount, ABiasCount: int32); override;
  end;

const
  TNNOptimizerClassList : array[ TNNOptimizerType ] of TNNOptimizerClass = (
    TNNSGD, TNNAdam, TNNRMSProp
  );


implementation

{ TNNSGD }

constructor TNNSGD.Create;
begin
  inherited Create( otSGD, 0.1 );
end;

procedure TNNSGD.UpdateParameters(AWeights, ABiases, AWeightGrads,
  ABiasGrads: PNNFloat; AWeightCount, ABiasCount: int32);
var
  i: int32;
begin
  for i := 0 to AWeightCount - 1 do begin
    AWeights^ := AWeights^ - FLearningRate * AWeightGrads^;
    Inc(AWeights);
    Inc(AWeightGrads);
  end;

  for i := 0 to ABiasCount - 1 do begin
    ABiases^ := ABiases^ - FLearningRate * ABiasGrads^;
    Inc(ABiases);
    Inc(ABiasGrads);
  end;
end;

{ TNNAdam }

procedure TNNAdam.Clear;
begin
  FT := 0;
  FBeta1 := 0.9;
  FBeta2 := 0.999;
  FEpsilon := 1e-8;
  FWeightCount := 0;
  FBiasCount := 0;
  FInited := false;
end;

constructor TNNAdam.Create;
begin
  inherited Create( otAdam, 0.001 );
end;

procedure TNNAdam.UpdateParameters(AWeights, ABiases, AWeightGrads,
  ABiasGrads: PNNFloat; AWeightCount, ABiasCount: int32);
var
  i: int32;
  m_hat, v_hat: NNFloat;
  beta1_t, beta2_t: NNFloat;
  WeightsM, WeightsV, BiasesM, BiasesV: PNNFloat;
begin
  // Инициализация моментов, если не сделано
  if not FInited then begin
    SetLength( FWeightsM, AWeightCount );
    SetLength( FWeightsV, AWeightCount );
    FillChar( FWeightsM[ 0 ], AWeightCount * SizeOf(NNFloat), 0 );
    FillChar( FWeightsV[ 0 ], AWeightCount * SizeOf(NNFloat), 0 );
    FWeightCount := AWeightCount;

    SetLength( FBiasesM, ABiasCount );
    SetLength( FBiasesV, ABiasCount );
    FillChar( FBiasesM[ 0 ], ABiasCount * SizeOf(NNFloat), 0 );
    FillChar( FBiasesV[ 0 ], ABiasCount * SizeOf(NNFloat), 0 );
    FBiasCount := ABiasCount;

    FInited := true;
  end;

  Inc(FT); // Увеличиваем счётчик шагов

  // Коррекция смещения для моментов
  beta1_t := Power(FBeta1, FT);
  beta2_t := Power(FBeta2, FT);

  // Копируем указатели для итерации
  WeightsM := @FWeightsM[ 0 ];
  WeightsV := @FWeightsV[ 0 ];
  BiasesM := @FBiasesM[ 0 ];
  BiasesV := @FBiasesV[ 0 ];

  // Обновление весов
  for i := 0 to AWeightCount - 1 do begin
    // Первый момент: m = beta1 * m + (1 - beta1) * grad
    WeightsM^ := FBeta1 * WeightsM^ + (1 - FBeta1) * AWeightGrads^;
    // Второй момент: v = beta2 * v + (1 - beta2) * grad^2
    WeightsV^ := FBeta2 * WeightsV^ + (1 - FBeta2) * Sqr(AWeightGrads^);
    // Коррекция смещения
    m_hat := WeightsM^ / (1 - beta1_t);
    v_hat := WeightsV^ / (1 - beta2_t);
    // Обновление веса
    AWeights^ := AWeights^ - FLearningRate * m_hat / (Sqrt(v_hat) + FEpsilon);
    Inc(AWeights);
    Inc(AWeightGrads);
    Inc(WeightsM);
    Inc(WeightsV);
  end;

  // Обновление смещений
  for i := 0 to ABiasCount - 1 do begin
    // Первый момент
    BiasesM^ := FBeta1 * BiasesM^ + (1 - FBeta1) * ABiasGrads^;
    // Второй момент
    BiasesV^ := FBeta2 * BiasesV^ + (1 - FBeta2) * Sqr(ABiasGrads^);
    // Коррекция смещения
    m_hat := BiasesM^ / (1 - beta1_t);
    v_hat := BiasesV^ / (1 - beta2_t);
    // Обновление смещения
    ABiases^ := ABiases^ - FLearningRate * m_hat / (Sqr(v_hat) + FEpsilon);
    Inc(ABiases);
    Inc(ABiasGrads);
    Inc(BiasesM);
    Inc(BiasesV);
  end;
end;

{ TNNRMSProp }

procedure TNNRMSProp.Clear;
begin
  FRho := 0.9;
  FEpsilon := 1e-8;
  FWeightCount := 0;
  FBiasCount := 0;
  FInited := false;
end;

constructor TNNRMSProp.Create;
begin
  inherited Create( otRMSProp, 0.001 );
end;

procedure TNNRMSProp.UpdateParameters(AWeights, ABiases, AWeightGrads,
  ABiasGrads: PNNFloat; AWeightCount, ABiasCount: int32);
var
  i: int32;
  v: NNFloat;
  WeightsV, BiasesV: PNNFloat;
begin
  // Инициализация v, если не сделано или размеры изменились
  if not FInited then begin
    SetLength( FWeightsV, AWeightCount );
    FillChar (FWeightsV[ 0 ], AWeightCount * SizeOf(NNFloat), 0 );
    FWeightCount := AWeightCount;

    SetLength( FBiasesV, ABiasCount );
    FillChar( FBiasesV[ 0 ], ABiasCount * SizeOf(NNFloat), 0);
    FBiasCount := ABiasCount;
  end;

  // Копируем указатели для итерации
  WeightsV := @FWeightsV[ 0 ];
  BiasesV := @FBiasesV[ 0 ];

  // Обновление весов
  for i := 0 to AWeightCount - 1 do begin
    // Среднее квадратов градиентов: v = rho * v + (1 - rho) * grad^2
    WeightsV^ := FRho * WeightsV^ + (1 - FRho) * Sqr(AWeightGrads^);
    // Обновление веса: w = w - learning_rate * grad / (sqrt(v) + epsilon)
    AWeights^ := AWeights^ - FLearningRate * AWeightGrads^ / (Sqrt(WeightsV^) + FEpsilon);
    Inc(AWeights);
    Inc(AWeightGrads);
    Inc(WeightsV);
  end;

  // Обновление смещений
  for i := 0 to ABiasCount - 1 do begin
    // Среднее квадратов градиентов
    BiasesV^ := FRho * BiasesV^ + (1 - FRho) * Sqr(ABiasGrads^);
    // Обновление смещения
    ABiases^ := ABiases^ - FLearningRate * ABiasGrads^ / (Sqrt(BiasesV^) + FEpsilon);
    Inc(ABiases);
    Inc(ABiasGrads);
    Inc(BiasesV);
  end;
end;

end.
